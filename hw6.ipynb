{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc95f30",
   "metadata": {},
   "source": [
    "# Homework 6: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfca0",
   "metadata": {},
   "source": [
    "The goals of this assignment are:\n",
    "1. Develop a better understanding of the *self-attention mechanism* in Transformers by implementing it in numpy. \n",
    "2. Understand and train a BERT-based model. \n",
    "3. Strengthen your understanding of using HuggingFace's `transformers` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05e52d",
   "metadata": {},
   "source": [
    "## Organization and Instructions\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins.\n",
    "\n",
    "**Part 1: Background.** \n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** \n",
    "- 2A. Self-attention \n",
    "- 2B. Zero-shot predictions \n",
    "- 2C. Fine-tuning \n",
    "\n",
    "\n",
    "**Addtional instructions.** \n",
    "- Please follow the 50-foot rule. Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "\n",
    "**Evaluation.** Your solution will be evaluated *manually* by the TAs and instructor. \n",
    "\n",
    "To help bridge the gap between previous homeworks and the final project. We are **not giving you an autograder**. We hope to help wean you off the grader and give you practice testing your own code.\n",
    "\n",
    "Please come see us during help hours if you need additional assistance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28832",
   "metadata": {},
   "source": [
    "## 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8ca2",
   "metadata": {},
   "source": [
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "88c48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77f509",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d637bf",
   "metadata": {},
   "source": [
    "#### Installing other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "41206a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import typing\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding)\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f0bc86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util #inspect util.py to see what is in this file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee52cb",
   "metadata": {},
   "source": [
    "## 1B. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430ccd4",
   "metadata": {},
   "source": [
    "In this homework, we will use the WinoGrande dataset. This is an **extremely challenging dataset** that even a BERT-based model might have **a lot of room for performance improvements!** \n",
    "\n",
    "You can read more about the dataset in [this paper](https://cdn.aaai.org/ojs/6399/6399-13-9624-1-10-20200517.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de9f3a",
   "metadata": {},
   "source": [
    "Here is Table 1 from the WinoGrande paper with examples:  \n",
    "\n",
    "![](figs/winograd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d855501",
   "metadata": {},
   "source": [
    "HuggingFace provides a Python package for loading (and uploading datasets). You can read more about the `datasets` Python package [here](https://huggingface.co/docs/datasets/en/index). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "825a7bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. train exs= 640\n",
      "Num. dev exs= 100\n"
     ]
    }
   ],
   "source": [
    "# Load the WinoGrande dataset\n",
    "dataset = load_dataset(\"allenai/winogrande\", \"winogrande_s\", trust_remote_code=True)\n",
    "\n",
    "# Access the training and validation splits\n",
    "train_dataset = dataset[\"train\"]\n",
    "validation_dataset = dataset[\"validation\"].select(range(100)) #We'll just look at 100 dev exs\n",
    "\n",
    "print(f\"Num. train exs= {len(train_dataset)}\")\n",
    "print(f\"Num. dev exs= {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de732df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'I had to read an entire story for class tomorrow. Luckily, the _ was short.', 'option1': 'story', 'option2': 'class', 'answer': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Let's look at one example from the validation dataset\n",
    "print(validation_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0a59e",
   "metadata": {},
   "source": [
    "Above, the `'sentence'` is the full sentence with a `_` for where the pronoun or noun options should go. \n",
    "\n",
    "Then `option1` and `option2` are the two token spans from the sentence the model will eventually choose from and `answer` is the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07013b71",
   "metadata": {},
   "source": [
    "## 2A. Self-attention\n",
    "\n",
    "In this part, you will implement the parallelized version of the *masked* self-attention mechanism in Transformers using only numpy.\n",
    "\n",
    "\n",
    "Recall, for each layer $k$ in the transformer block we have "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58916e6f",
   "metadata": {},
   "source": [
    "For a single example with $n$ tokens and embedding dimension $d$, we first have $X^k$, the contextual embedding matrix (size $n\\times d$) for layer $k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a4705",
   "metadata": {},
   "source": [
    "Then, we introduce the weights, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d6663",
   "metadata": {},
   "source": [
    "$$ Q = X^k \\times W_Q$$ \n",
    "$$ K = X^k \\times W_K$$\n",
    "$$ V = X^k \\times W_V $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b965e22",
   "metadata": {},
   "source": [
    "and use the new matrices to get the contextual embedding matrix for the next layer, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c218",
   "metadata": {},
   "source": [
    "$$ X^{k+1} = \\text{softmax} \\bigg( \\text{mask} \\bigg( \\frac{QK^T}{\\sqrt{d}} \\bigg) \\bigg) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3600819",
   "metadata": {},
   "source": [
    "This is computationally efficient in a matrix-multiplication-optimized library like `numpy` because it should have **no for-loops!** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601540b0",
   "metadata": {},
   "source": [
    "Let's implement self-attention for the (modified) example we were looking at in Part 1 \n",
    "\n",
    "*\"I had to read an entire story for class tomorrow. Luckily, it was short.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "44e41436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for our example \n",
    "toks = [\"i\", \"had\", \"to\", \"read\", \"an\", \n",
    "        \"entire\", \"story\", \"for\", \"class\", \"tomorrow\", \".\",\n",
    "       \"luckily\", \"it\", \"was\", \"short\", \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ba4bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-specified embeddings and weights (for testing)\n",
    "X, W_Q, W_K, W_V = util.load_attention_data(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a8fb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement your approach in this function\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def self_attention(X: np.ndarray, W_Q: np.ndarray, \n",
    "                   W_K: np.ndarray, W_V: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Implements (masked) self-attention mechanism for a single layer \n",
    "    (and a single example)\n",
    "    \n",
    "    Returns: X_new, a np.ndarray that is the same shape as X\n",
    "    \n",
    "    Notes:\n",
    "    - You can only use numpy for this part of the homework and no other packages\n",
    "    - Your solution must not have any for-loops!\n",
    "    \n",
    "    Tips: \n",
    "        - Double-check the shapes of all the matrices you're working with. \n",
    "        - We recommend making a helper function for the softmax.\n",
    "        - You may a subset of these numpy methods and operators helpful: `np.exp`, `@`, `np.triu_indicies`, `np.reshape`, `np.inf`, `np.broadcast_to`, `np.choose`.  \n",
    "    \"\"\"\n",
    "    Q = X @ W_Q \n",
    "    K = X @ W_K  \n",
    "    V = X @ W_V  \n",
    "    \n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "   \n",
    "    scores = (Q @ K.T) / np.sqrt(d)\n",
    "\n",
    "    indexes = np.triu_indices(n, k = 1)\n",
    "    scores[indexes] = -np.inf\n",
    "   \n",
    "    attention_weights = softmax(scores)  \n",
    "\n",
    "    X_new = attention_weights @ V  \n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "26c1797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = self_attention(X, W_Q, W_K, W_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d36bd",
   "metadata": {},
   "source": [
    "## 2B. Zero-shot predictions\n",
    "\n",
    "Now, we will use a distilled version of \"RoBERTa\" (a BERT variant) to make zero-shot predictions on the WinoGrande dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d696c",
   "metadata": {},
   "source": [
    "#### Tokenization and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac9c4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c2f34eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23e1844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sarah was a much better surgeon than Maria so _ always got the easier cases.'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First example in the dev dataset (see dataset loading in Part 1B above)\n",
    "text1 = validation_dataset[0]['sentence']\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fe9c484b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 4532, 2001, 1037, 2172, 2488, 9431, 2084, 3814, 2061, 1035, 2467,\n",
       "         2288, 1996, 6082, 3572, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
       "         [ 0,  5],\n",
       "         [ 6,  9],\n",
       "         [10, 11],\n",
       "         [12, 16],\n",
       "         [17, 23],\n",
       "         [24, 31],\n",
       "         [32, 36],\n",
       "         [37, 42],\n",
       "         [43, 45],\n",
       "         [46, 47],\n",
       "         [48, 54],\n",
       "         [55, 58],\n",
       "         [59, 62],\n",
       "         [63, 69],\n",
       "         [70, 75],\n",
       "         [75, 76],\n",
       "         [ 0,  0]]])}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts to tokens and attention mask \n",
    "# The attention mask will be 0 if there are special \"PAD\" tokens\n",
    "# offset_mapping gives the start and end character index for each token in the original text \n",
    "inputs = tokenizer(text1, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c5daad8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'sarah',\n",
       " 'was',\n",
       " 'a',\n",
       " 'much',\n",
       " 'better',\n",
       " 'surgeon',\n",
       " 'than',\n",
       " 'maria',\n",
       " 'so',\n",
       " '_',\n",
       " 'always',\n",
       " 'got',\n",
       " 'the',\n",
       " 'easier',\n",
       " 'cases',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the tokens \n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bbcaf",
   "metadata": {},
   "source": [
    "Note: Above, when we see the `###` characters before tokens, this means the BERT tokenizer has split a word into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9e6ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the function below \n",
    "def which_tok_index(tok_string: str, sentence: str, inputs: torch.tensor) -> int:\n",
    "    \"\"\"\n",
    "    Inputs: the token string (tok_string) of interest, the original sentence\n",
    "    and the tokenized input tensors \n",
    "    \n",
    "    Returns: (int) the first index that matches the tok_string. \n",
    "    \n",
    "        If tok_string gets tokenized into multiple tokens, return the *first* index corresponding\n",
    "        to the multi-token span. \n",
    "\n",
    "        If there is no match (which could happen), return 0. \n",
    "    \n",
    "    Example: \n",
    "        tok_string=\"sarah\"\n",
    "        \n",
    "        input_ids = {'input_ids': tensor([[ 101, 4532, 2001, 1037, 2172, 2488, 9431, 2084, 3814, 2061, 1035, 2467,\n",
    "         2288, 1996, 6082, 3572, 1012,  102]]), \n",
    "                     'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), \n",
    "                     'offset_mapping': tensor([[[ 0,  0],\n",
    "                     [ 0,  5],\n",
    "                     [ 6,  9],\n",
    "                     [10, 11],\n",
    "                     [12, 16],\n",
    "                     [17, 23],\n",
    "                     [24, 31],\n",
    "                     [32, 36],\n",
    "                     [37, 42],\n",
    "                     [43, 45],\n",
    "                     [46, 47],\n",
    "                     [48, 54],\n",
    "                     [55, 58],\n",
    "                     [59, 62],\n",
    "                     [63, 69],\n",
    "                     [70, 75],\n",
    "                     [75, 76],\n",
    "                     [ 0,  0]]])}\n",
    "        \n",
    "        Returns: 1 \n",
    "        \n",
    "        This example returns 1 since the token id at index 1 (value 432)\n",
    "        starts at character index 6 in the orginal sentence and corresponds to \"sarah.\"\n",
    "        \n",
    "    Tips:  \n",
    "        - Understaning 'offset_mapping' and Python string methods may be helpful here\n",
    "        - tokenizer.convert_ids_to_tokens could help with debugging \n",
    "    \"\"\"\n",
    "    start_idx = sentence.find(tok_string)\n",
    "    \n",
    "    if start_idx == -1:\n",
    "        return 0  \n",
    "    \n",
    "    offset_mapping = inputs['offset_mapping'][0]\n",
    "    \n",
    "    for i, (token_start, token_end) in enumerate(offset_mapping):\n",
    "        if token_start == start_idx and token_end != 0:\n",
    "            return i\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dca1f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 == 1?\n",
      "8 == 8?\n",
      "10 == 10?\n"
     ]
    }
   ],
   "source": [
    "# Unit test\n",
    "sent = validation_dataset[0]['sentence']\n",
    "inputs = tokenizer(sent, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(\"sarah\", sent.lower(), inputs)\n",
    "print(t1, \"== 1?\")\n",
    "t2 = which_tok_index(\"maria\", sent.lower(), inputs)\n",
    "print(t2, \"== 8?\")\n",
    "t3 = which_tok_index(\"_\", sent.lower(), inputs)\n",
    "print(t3, \"== 10?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c2d4acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 == 7?\n",
      "12 == 12?\n",
      "0 == 0?\n"
     ]
    }
   ],
   "source": [
    "# Another unit test \n",
    "sent3 = validation_dataset[3]['sentence']\n",
    "inputs3 = tokenizer(sent3, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "\n",
    "t1 = which_tok_index(validation_dataset[3]['option1'].lower(), sent3, inputs3)\n",
    "print(t1, \"== 7?\")\n",
    "t2 = which_tok_index(validation_dataset[3]['option2'].lower(), sent3, inputs3)\n",
    "print(t2, \"== 12?\")\n",
    "t3 = which_tok_index(\"blah\", sent3, inputs3)\n",
    "print(t3, \"== 0?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02109352",
   "metadata": {},
   "source": [
    "#### Zero-shot prediction\n",
    "\n",
    "Now, we'll use the model to make zero-shot predictions. Note, this is \"zero-shot\" because we haven't ever trained the model on this particular task or dataset.  \n",
    "\n",
    "Here's how we will make zero-shot predictions: \n",
    "1. Pass the sentence (after tokenization) into the pre-trained model \n",
    "2. Obtain the final layer contextual embeddings for the `\"_\"` token as well as the *first* token for the substring of `option1` and likewise for `option2`. \n",
    "3. Find the cosine similarity between these contextual embeddings between `\"_\"` and the embedding we chose for `option1` and likewise for `option2`. \n",
    "4. Whichever has the higher cosine similarity (`option1` or `option2`), make this the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "73528eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation\n",
    "def zero_shot_predictions(model, tokenizer, dataset) -> List[int]: \n",
    "    \"\"\"\n",
    "    Make zero-shot predictions with the last layer contextual embedding\n",
    "    cosine similarity method described in the previous cell. \n",
    "    \n",
    "    Returns: \n",
    "        List[str], a list of strings, one element for each \n",
    "        example in the input dataset. Each element is an int: \n",
    "            - 1 corresponding to \"option1\" in the dataset\n",
    "            - 2 corresponding to \"option2\" in the dataset\n",
    "    \n",
    "    Note: \n",
    "        - For now, it's ok if you have a for-loop over examples. \n",
    "          (In an actual industry setting, you would make this all parallelized)\n",
    "        - You might make use of the `which_tok_index()` helper function \n",
    "        you just implemented \n",
    "        - The documentation on AutoModelForSequenceClassification may be helpful here. \n",
    "        - torch.nn.functional may have some helpful methods \n",
    "        - Using model.eval() and torch.no_grad() will speed things up (since Pytorch will not\n",
    "            have to make the computation graph)\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in dataset:\n",
    "            sentence = example[\"sentence\"]\n",
    "            option1 = example[\"option1\"]\n",
    "            option2 = example[\"option2\"]\n",
    "        \n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "            \n",
    "            blank_index = which_tok_index(\"_\", sentence, inputs)\n",
    "            option1_index = which_tok_index(option1, sentence, inputs)\n",
    "            option2_index = which_tok_index(option2, sentence, inputs)\n",
    "\n",
    "            inputs.pop(\"offset_mapping\")\n",
    "            outputs = model(**inputs, output_hidden_states = True)\n",
    "            last_hidden_states = outputs.hidden_states[-1][0] \n",
    "\n",
    "            blank_embedding = last_hidden_states[blank_index]\n",
    "            option1_embedding = last_hidden_states[option1_index]\n",
    "            option2_embedding = last_hidden_states[option2_index]\n",
    "            \n",
    "            similarity1 = F.cosine_similarity(blank_embedding, option1_embedding, dim=0)\n",
    "            similarity2 = F.cosine_similarity(blank_embedding, option2_embedding, dim=0)\n",
    "            \n",
    "            prediction = 1 if similarity1 > similarity2 else 2\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7ced1ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# You might see a warning below. \n",
    "# We'll end up doing this in the next part of the homework;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa4ccdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code on just a single example \n",
    "zero_shot_predictions(model, tokenizer, validation_dataset.select(range(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "97d45887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the full predictions \n",
    "preds = zero_shot_predictions(model, tokenizer, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "922bac88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and check the following\n",
    "len(preds) == len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02e9e2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3daaf618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 of baseline (maj. class)= 0.67\n",
      "F1 of zero-shot= 0.43\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    truth = [int(x['answer']) for x in validation_dataset]\n",
    "    assert len(truth) == len(preds)\n",
    "    y_true = np.array(truth)\n",
    "    y_pred = np.array(preds)\n",
    "    y_baseline = np.ones(len(y_true)) *2\n",
    "    print(\"F1 of baseline (maj. class)=\", np.round(f1_score(y_true, y_baseline, pos_label=2), 2))\n",
    "    print(\"F1 of zero-shot=\", np.round(f1_score(y_true, y_pred, pos_label=2), 2))\n",
    "except: \n",
    "    print(\"Need preds to be equal to truth for eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45446669",
   "metadata": {},
   "source": [
    "**What do you attribute to the zero-shot model's performance?** \n",
    "\n",
    "The nature of the task for this data set largely depends on the model somehow \"understanding\" the meanings of words like 'because' and 'but'(in the case of second example). Since the input is just the sentence(which is relatively short) and the model doesn't have much context to work with it is not effectively understand the meanings of the words that change between the two sentences. The model is too general so it's not doing well because this specific task (specifically on this dataset). reqThe model might have done better if we had more context on either side of the transition words(like if we had a paragraph on each side). It might also have done better if we had more layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009bbae",
   "metadata": {},
   "source": [
    "##  2C. Extra Credit: Fine-tuning\n",
    "\n",
    "Try fine-tuning a model on this training datasets (or other training datasets). Can you do better than the zero-shot model? \n",
    "\n",
    "Tips:\n",
    "- [This](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) HuggingFace tutorial may be helpful for learning the transformers syntax. \n",
    "- Think **carefully** about the dataset and task when you're reasoning about the model's performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "436a4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7260c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85190985ddff43c8971e0232fa7e6257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'processing_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_data[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_data[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     processing_class\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m      7\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'processing_class'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f93ec2",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6e0b5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found notebook file, creating submission zip...\n",
      "  adding: hw6.ipynb (deflated 73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw6.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw6.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
